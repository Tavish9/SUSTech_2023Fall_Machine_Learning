{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB7 Assignment\n",
    "> The document description are designed by JIa Yanhong in 2022. Oct. 20th\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAB Assignment\n",
    "### Exercise 1 logistic regression (20 points )\n",
    "This exercise uses dataset digit01.csv , which has 13 columns, and the last column is the dependent variable. \n",
    "\n",
    "This part requires you to implement a `logistic regression` using the pytorch framework (defining a logistic regression class that inherits `nn.module`). To test your model, we provide a dataset `digit01.csv` which is in the **datasets folder**. This dataset requires you to divide the training set and the test set by yourself, and it is recommended that 80% of the training set and 20% of the test set be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   10  11  12\n",
       "0   1   1   1   1   0   1   1   0   1   1   1   1   0\n",
       "1   0   1   1   1   0   1   1   0   1   1   1   1   0\n",
       "2   1   1   0   1   0   1   1   0   1   1   1   1   0\n",
       "3   1   1   1   1   0   1   1   0   1   1   1   0   0\n",
       "4   1   1   1   1   0   1   1   0   1   0   1   1   0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/digit01.csv', header=None)\n",
    "df.head()\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Splitting dataset into 80% Training and 20% Testing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([51, 12]),\n",
       " torch.Size([13, 12]),\n",
       " torch.Size([51]),\n",
       " torch.Size([13]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "y = df.iloc[:, -1].values\n",
    "X = df.iloc[:, :-1].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_test = torch.from_numpy(y_test).long()\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define a LogisticRegression subclass of nn. Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a LogisticRegression subclass of nn. Module.\n",
    "########### Write Your Code Here ###########\n",
    "from torch import nn\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(12, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=12, out_features=2, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "########### Write Your Code Here ###########\n",
    "model = LogisticRegression()\n",
    "model\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from torch import optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss = 0.7068455815315247\n",
      "epoch: 20, loss = 0.7061322331428528\n",
      "epoch: 40, loss = 0.705421507358551\n",
      "epoch: 60, loss = 0.7047134637832642\n",
      "epoch: 80, loss = 0.7040078639984131\n",
      "epoch: 100, loss = 0.7033049464225769\n",
      "epoch: 120, loss = 0.7026045322418213\n",
      "epoch: 140, loss = 0.701906681060791\n",
      "epoch: 160, loss = 0.7012115120887756\n",
      "epoch: 180, loss = 0.7005189061164856\n",
      "epoch: 200, loss = 0.6998287439346313\n",
      "epoch: 220, loss = 0.6991412043571472\n",
      "epoch: 240, loss = 0.6984561681747437\n",
      "epoch: 260, loss = 0.697773814201355\n",
      "epoch: 280, loss = 0.6970937848091125\n",
      "epoch: 300, loss = 0.696416437625885\n",
      "epoch: 320, loss = 0.6957415342330933\n",
      "epoch: 340, loss = 0.6950693130493164\n",
      "epoch: 360, loss = 0.6943994164466858\n",
      "epoch: 380, loss = 0.6937321424484253\n",
      "epoch: 400, loss = 0.6930673718452454\n",
      "epoch: 420, loss = 0.6924050450325012\n",
      "epoch: 440, loss = 0.6917451620101929\n",
      "epoch: 460, loss = 0.6910878419876099\n",
      "epoch: 480, loss = 0.6904329657554626\n",
      "epoch: 500, loss = 0.6897803544998169\n",
      "epoch: 520, loss = 0.689130425453186\n",
      "epoch: 540, loss = 0.688482940196991\n",
      "epoch: 560, loss = 0.6878377795219421\n",
      "epoch: 580, loss = 0.6871951222419739\n",
      "epoch: 600, loss = 0.6865547895431519\n",
      "epoch: 620, loss = 0.6859169006347656\n",
      "epoch: 640, loss = 0.6852813959121704\n",
      "epoch: 660, loss = 0.684648334980011\n",
      "epoch: 680, loss = 0.684017539024353\n",
      "epoch: 700, loss = 0.6833891868591309\n",
      "epoch: 720, loss = 0.6827630996704102\n",
      "epoch: 740, loss = 0.6821392774581909\n",
      "epoch: 760, loss = 0.6815179586410522\n",
      "epoch: 780, loss = 0.6808990240097046\n",
      "epoch: 800, loss = 0.6802821159362793\n",
      "epoch: 820, loss = 0.6796677708625793\n",
      "epoch: 840, loss = 0.6790556311607361\n",
      "epoch: 860, loss = 0.6784456372261047\n",
      "epoch: 880, loss = 0.6778380274772644\n",
      "epoch: 900, loss = 0.677232563495636\n",
      "epoch: 920, loss = 0.6766294836997986\n",
      "epoch: 940, loss = 0.6760284304618835\n",
      "epoch: 960, loss = 0.6754297614097595\n",
      "epoch: 980, loss = 0.6748332381248474\n",
      "epoch: 1000, loss = 0.674238920211792\n",
      "epoch: 1020, loss = 0.6736467480659485\n",
      "epoch: 1040, loss = 0.6730567216873169\n",
      "epoch: 1060, loss = 0.6724689602851868\n",
      "epoch: 1080, loss = 0.6718831658363342\n",
      "epoch: 1100, loss = 0.6712995767593384\n",
      "epoch: 1120, loss = 0.6707181930541992\n",
      "epoch: 1140, loss = 0.6701387763023376\n",
      "epoch: 1160, loss = 0.669561505317688\n",
      "epoch: 1180, loss = 0.6689862012863159\n",
      "epoch: 1200, loss = 0.6684132218360901\n",
      "epoch: 1220, loss = 0.6678420901298523\n",
      "epoch: 1240, loss = 0.6672730445861816\n",
      "epoch: 1260, loss = 0.6667060852050781\n",
      "epoch: 1280, loss = 0.666141152381897\n",
      "epoch: 1300, loss = 0.6655780673027039\n",
      "epoch: 1320, loss = 0.6650171279907227\n",
      "epoch: 1340, loss = 0.6644580960273743\n",
      "epoch: 1360, loss = 0.6639010906219482\n",
      "epoch: 1380, loss = 0.6633459329605103\n",
      "epoch: 1400, loss = 0.6627928018569946\n",
      "epoch: 1420, loss = 0.6622413992881775\n",
      "epoch: 1440, loss = 0.661692202091217\n",
      "epoch: 1460, loss = 0.6611447930335999\n",
      "epoch: 1480, loss = 0.6605992317199707\n",
      "epoch: 1500, loss = 0.6600555777549744\n",
      "epoch: 1520, loss = 0.6595138907432556\n",
      "epoch: 1540, loss = 0.6589739322662354\n",
      "epoch: 1560, loss = 0.6584357619285583\n",
      "epoch: 1580, loss = 0.6578994393348694\n",
      "epoch: 1600, loss = 0.6573650240898132\n",
      "epoch: 1620, loss = 0.6568324565887451\n",
      "epoch: 1640, loss = 0.6563015580177307\n",
      "epoch: 1660, loss = 0.6557723879814148\n",
      "epoch: 1680, loss = 0.6552451848983765\n",
      "epoch: 1700, loss = 0.6547195315361023\n",
      "epoch: 1720, loss = 0.6541957259178162\n",
      "epoch: 1740, loss = 0.6536737084388733\n",
      "epoch: 1760, loss = 0.6531533002853394\n",
      "epoch: 1780, loss = 0.6526346206665039\n",
      "epoch: 1800, loss = 0.6521176099777222\n",
      "epoch: 1820, loss = 0.6516023278236389\n",
      "epoch: 1840, loss = 0.6510887742042542\n",
      "epoch: 1860, loss = 0.6505767703056335\n",
      "epoch: 1880, loss = 0.6500665545463562\n",
      "epoch: 1900, loss = 0.6495577692985535\n",
      "epoch: 1920, loss = 0.6490507125854492\n",
      "epoch: 1940, loss = 0.6485452651977539\n",
      "epoch: 1960, loss = 0.6480413675308228\n",
      "epoch: 1980, loss = 0.6475390791893005\n",
      "epoch: 2000, loss = 0.647038459777832\n",
      "epoch: 2020, loss = 0.6465394496917725\n",
      "epoch: 2040, loss = 0.6460416913032532\n",
      "epoch: 2060, loss = 0.6455457210540771\n",
      "epoch: 2080, loss = 0.6450512409210205\n",
      "epoch: 2100, loss = 0.644558310508728\n",
      "epoch: 2120, loss = 0.6440668106079102\n",
      "epoch: 2140, loss = 0.6435768008232117\n",
      "epoch: 2160, loss = 0.6430884003639221\n",
      "epoch: 2180, loss = 0.642601490020752\n",
      "epoch: 2200, loss = 0.6421159505844116\n",
      "epoch: 2220, loss = 0.6416319012641907\n",
      "epoch: 2240, loss = 0.6411491632461548\n",
      "epoch: 2260, loss = 0.6406680941581726\n",
      "epoch: 2280, loss = 0.6401883363723755\n",
      "epoch: 2300, loss = 0.639710009098053\n",
      "epoch: 2320, loss = 0.6392329931259155\n",
      "epoch: 2340, loss = 0.6387575268745422\n",
      "epoch: 2360, loss = 0.638283371925354\n",
      "epoch: 2380, loss = 0.6378105878829956\n",
      "epoch: 2400, loss = 0.6373391151428223\n",
      "epoch: 2420, loss = 0.6368690729141235\n",
      "epoch: 2440, loss = 0.6364003419876099\n",
      "epoch: 2460, loss = 0.6359329223632812\n",
      "epoch: 2480, loss = 0.6354669332504272\n",
      "epoch: 2500, loss = 0.6350021362304688\n",
      "epoch: 2520, loss = 0.6345387101173401\n",
      "epoch: 2540, loss = 0.6340765357017517\n",
      "epoch: 2560, loss = 0.6336156129837036\n",
      "epoch: 2580, loss = 0.6331561207771301\n",
      "epoch: 2600, loss = 0.6326978206634521\n",
      "epoch: 2620, loss = 0.6322407722473145\n",
      "epoch: 2640, loss = 0.6317849159240723\n",
      "epoch: 2660, loss = 0.6313303709030151\n",
      "epoch: 2680, loss = 0.6308770775794983\n",
      "epoch: 2700, loss = 0.6304250359535217\n",
      "epoch: 2720, loss = 0.6299741268157959\n",
      "epoch: 2740, loss = 0.6295244693756104\n",
      "epoch: 2760, loss = 0.6290759444236755\n",
      "epoch: 2780, loss = 0.628628671169281\n",
      "epoch: 2800, loss = 0.6281825304031372\n",
      "epoch: 2820, loss = 0.6277375817298889\n",
      "epoch: 2840, loss = 0.6272938847541809\n",
      "epoch: 2860, loss = 0.6268512010574341\n",
      "epoch: 2880, loss = 0.6264097690582275\n",
      "epoch: 2900, loss = 0.6259694695472717\n",
      "epoch: 2920, loss = 0.6255303025245667\n",
      "epoch: 2940, loss = 0.6250922083854675\n",
      "epoch: 2960, loss = 0.6246552467346191\n",
      "epoch: 2980, loss = 0.6242194175720215\n",
      "epoch: 3000, loss = 0.6237846612930298\n",
      "epoch: 3020, loss = 0.6233510971069336\n",
      "epoch: 3040, loss = 0.6229184865951538\n",
      "epoch: 3060, loss = 0.62248694896698\n",
      "epoch: 3080, loss = 0.6220566034317017\n",
      "epoch: 3100, loss = 0.6216272115707397\n",
      "epoch: 3120, loss = 0.6211990118026733\n",
      "epoch: 3140, loss = 0.6207717657089233\n",
      "epoch: 3160, loss = 0.6203455328941345\n",
      "epoch: 3180, loss = 0.6199203133583069\n",
      "epoch: 3200, loss = 0.6194961667060852\n",
      "epoch: 3220, loss = 0.6190730929374695\n",
      "epoch: 3240, loss = 0.6186510324478149\n",
      "epoch: 3260, loss = 0.6182299256324768\n",
      "epoch: 3280, loss = 0.6178099513053894\n",
      "epoch: 3300, loss = 0.6173908114433289\n",
      "epoch: 3320, loss = 0.6169726848602295\n",
      "epoch: 3340, loss = 0.6165555715560913\n",
      "epoch: 3360, loss = 0.6161394715309143\n",
      "epoch: 3380, loss = 0.6157242059707642\n",
      "epoch: 3400, loss = 0.61531001329422\n",
      "epoch: 3420, loss = 0.614896833896637\n",
      "epoch: 3440, loss = 0.614484429359436\n",
      "epoch: 3460, loss = 0.6140730977058411\n",
      "epoch: 3480, loss = 0.6136627197265625\n",
      "epoch: 3500, loss = 0.6132532954216003\n",
      "epoch: 3520, loss = 0.6128446459770203\n",
      "epoch: 3540, loss = 0.6124370098114014\n",
      "epoch: 3560, loss = 0.6120303273200989\n",
      "epoch: 3580, loss = 0.611624538898468\n",
      "epoch: 3600, loss = 0.611219584941864\n",
      "epoch: 3620, loss = 0.6108155846595764\n",
      "epoch: 3640, loss = 0.6104124784469604\n",
      "epoch: 3660, loss = 0.6100103259086609\n",
      "epoch: 3680, loss = 0.6096089482307434\n",
      "epoch: 3700, loss = 0.6092085242271423\n",
      "epoch: 3720, loss = 0.6088088750839233\n",
      "epoch: 3740, loss = 0.608410120010376\n",
      "epoch: 3760, loss = 0.608012318611145\n",
      "epoch: 3780, loss = 0.6076152920722961\n",
      "epoch: 3800, loss = 0.6072191596031189\n",
      "epoch: 3820, loss = 0.6068238615989685\n",
      "epoch: 3840, loss = 0.6064293384552002\n",
      "epoch: 3860, loss = 0.6060357093811035\n",
      "epoch: 3880, loss = 0.6056429147720337\n",
      "epoch: 3900, loss = 0.6052509546279907\n",
      "epoch: 3920, loss = 0.6048597693443298\n",
      "epoch: 3940, loss = 0.6044695377349854\n",
      "epoch: 3960, loss = 0.6040799617767334\n",
      "epoch: 3980, loss = 0.6036912798881531\n",
      "epoch: 4000, loss = 0.6033034324645996\n",
      "epoch: 4020, loss = 0.6029162406921387\n",
      "epoch: 4040, loss = 0.6025300025939941\n",
      "epoch: 4060, loss = 0.6021444797515869\n",
      "epoch: 4080, loss = 0.6017597913742065\n",
      "epoch: 4100, loss = 0.6013758182525635\n",
      "epoch: 4120, loss = 0.6009926199913025\n",
      "epoch: 4140, loss = 0.6006102561950684\n",
      "epoch: 4160, loss = 0.6002286672592163\n",
      "epoch: 4180, loss = 0.5998477339744568\n",
      "epoch: 4200, loss = 0.5994676947593689\n",
      "epoch: 4220, loss = 0.5990884304046631\n",
      "epoch: 4240, loss = 0.598709762096405\n",
      "epoch: 4260, loss = 0.5983319878578186\n",
      "epoch: 4280, loss = 0.5979548692703247\n",
      "epoch: 4300, loss = 0.5975785851478577\n",
      "epoch: 4320, loss = 0.5972029566764832\n",
      "epoch: 4340, loss = 0.5968281626701355\n",
      "epoch: 4360, loss = 0.5964540839195251\n",
      "epoch: 4380, loss = 0.5960807204246521\n",
      "epoch: 4400, loss = 0.5957080125808716\n",
      "epoch: 4420, loss = 0.5953360795974731\n",
      "epoch: 4440, loss = 0.594964861869812\n",
      "epoch: 4460, loss = 0.5945942997932434\n",
      "epoch: 4480, loss = 0.5942245721817017\n",
      "epoch: 4500, loss = 0.5938555598258972\n",
      "epoch: 4520, loss = 0.5934872031211853\n",
      "epoch: 4540, loss = 0.5931194424629211\n",
      "epoch: 4560, loss = 0.5927524566650391\n",
      "epoch: 4580, loss = 0.5923862457275391\n",
      "epoch: 4600, loss = 0.5920206904411316\n",
      "epoch: 4620, loss = 0.5916557908058167\n",
      "epoch: 4640, loss = 0.591291606426239\n",
      "epoch: 4660, loss = 0.5909281373023987\n",
      "epoch: 4680, loss = 0.5905652046203613\n",
      "epoch: 4700, loss = 0.5902031064033508\n",
      "epoch: 4720, loss = 0.5898416042327881\n",
      "epoch: 4740, loss = 0.5894808173179626\n",
      "epoch: 4760, loss = 0.5891206860542297\n",
      "epoch: 4780, loss = 0.5887612104415894\n",
      "epoch: 4800, loss = 0.5884024500846863\n",
      "epoch: 4820, loss = 0.588044285774231\n",
      "epoch: 4840, loss = 0.5876867771148682\n",
      "epoch: 4860, loss = 0.5873299241065979\n",
      "epoch: 4880, loss = 0.5869737863540649\n",
      "epoch: 4900, loss = 0.5866183042526245\n",
      "epoch: 4920, loss = 0.5862634778022766\n",
      "epoch: 4940, loss = 0.5859091877937317\n",
      "epoch: 4960, loss = 0.5855555534362793\n",
      "epoch: 4980, loss = 0.585202693939209\n",
      "epoch: 5000, loss = 0.5848504304885864\n",
      "epoch: 5020, loss = 0.5844987630844116\n",
      "epoch: 5040, loss = 0.5841477513313293\n",
      "epoch: 5060, loss = 0.5837973356246948\n",
      "epoch: 5080, loss = 0.5834476351737976\n",
      "epoch: 5100, loss = 0.5830984115600586\n",
      "epoch: 5120, loss = 0.5827499032020569\n",
      "epoch: 5140, loss = 0.5824021100997925\n",
      "epoch: 5160, loss = 0.582054853439331\n",
      "epoch: 5180, loss = 0.5817081928253174\n",
      "epoch: 5200, loss = 0.5813621282577515\n",
      "epoch: 5220, loss = 0.5810167789459229\n",
      "epoch: 5240, loss = 0.5806719660758972\n",
      "epoch: 5260, loss = 0.5803277492523193\n",
      "epoch: 5280, loss = 0.579984188079834\n",
      "epoch: 5300, loss = 0.5796412229537964\n",
      "epoch: 5320, loss = 0.5792988538742065\n",
      "epoch: 5340, loss = 0.5789571404457092\n",
      "epoch: 5360, loss = 0.5786159038543701\n",
      "epoch: 5380, loss = 0.5782754421234131\n",
      "epoch: 5400, loss = 0.5779353976249695\n",
      "epoch: 5420, loss = 0.577596127986908\n",
      "epoch: 5440, loss = 0.5772573351860046\n",
      "epoch: 5460, loss = 0.5769191980361938\n",
      "epoch: 5480, loss = 0.576581597328186\n",
      "epoch: 5500, loss = 0.576244592666626\n",
      "epoch: 5520, loss = 0.5759081840515137\n",
      "epoch: 5540, loss = 0.5755723118782043\n",
      "epoch: 5560, loss = 0.5752370357513428\n",
      "epoch: 5580, loss = 0.5749025344848633\n",
      "epoch: 5600, loss = 0.5745683312416077\n",
      "epoch: 5620, loss = 0.5742348432540894\n",
      "epoch: 5640, loss = 0.5739019513130188\n",
      "epoch: 5660, loss = 0.5735695362091064\n",
      "epoch: 5680, loss = 0.5732378363609314\n",
      "epoch: 5700, loss = 0.5729066133499146\n",
      "epoch: 5720, loss = 0.5725759863853455\n",
      "epoch: 5740, loss = 0.5722458958625793\n",
      "epoch: 5760, loss = 0.571916401386261\n",
      "epoch: 5780, loss = 0.5715875625610352\n",
      "epoch: 5800, loss = 0.5712591409683228\n",
      "epoch: 5820, loss = 0.5709313154220581\n",
      "epoch: 5840, loss = 0.5706040859222412\n",
      "epoch: 5860, loss = 0.5702773928642273\n",
      "epoch: 5880, loss = 0.5699513554573059\n",
      "epoch: 5900, loss = 0.5696257948875427\n",
      "epoch: 5920, loss = 0.5693007707595825\n",
      "epoch: 5940, loss = 0.5689763426780701\n",
      "epoch: 5960, loss = 0.5686524510383606\n",
      "epoch: 5980, loss = 0.5683290958404541\n",
      "epoch: 6000, loss = 0.5680063366889954\n",
      "epoch: 6020, loss = 0.5676841139793396\n",
      "epoch: 6040, loss = 0.5673624873161316\n",
      "epoch: 6060, loss = 0.5670413374900818\n",
      "epoch: 6080, loss = 0.5667206645011902\n",
      "epoch: 6100, loss = 0.5664007067680359\n",
      "epoch: 6120, loss = 0.566081166267395\n",
      "epoch: 6140, loss = 0.5657622218132019\n",
      "epoch: 6160, loss = 0.5654438138008118\n",
      "epoch: 6180, loss = 0.5651258826255798\n",
      "epoch: 6200, loss = 0.5648085474967957\n",
      "epoch: 6220, loss = 0.5644917488098145\n",
      "epoch: 6240, loss = 0.5641754865646362\n",
      "epoch: 6260, loss = 0.563859760761261\n",
      "epoch: 6280, loss = 0.5635445713996887\n",
      "epoch: 6300, loss = 0.5632298588752747\n",
      "epoch: 6320, loss = 0.5629158616065979\n",
      "epoch: 6340, loss = 0.5626022219657898\n",
      "epoch: 6360, loss = 0.5622891187667847\n",
      "epoch: 6380, loss = 0.5619766116142273\n",
      "epoch: 6400, loss = 0.5616646409034729\n",
      "epoch: 6420, loss = 0.5613530874252319\n",
      "epoch: 6440, loss = 0.5610421895980835\n",
      "epoch: 6460, loss = 0.5607317686080933\n",
      "epoch: 6480, loss = 0.5604219436645508\n",
      "epoch: 6500, loss = 0.5601125359535217\n",
      "epoch: 6520, loss = 0.5598036646842957\n",
      "epoch: 6540, loss = 0.5594953894615173\n",
      "epoch: 6560, loss = 0.5591875314712524\n",
      "epoch: 6580, loss = 0.5588802099227905\n",
      "epoch: 6600, loss = 0.5585734844207764\n",
      "epoch: 6620, loss = 0.5582671761512756\n",
      "epoch: 6640, loss = 0.5579614639282227\n",
      "epoch: 6660, loss = 0.5576562881469727\n",
      "epoch: 6680, loss = 0.5573515892028809\n",
      "epoch: 6700, loss = 0.5570473670959473\n",
      "epoch: 6720, loss = 0.5567436814308167\n",
      "epoch: 6740, loss = 0.5564405918121338\n",
      "epoch: 6760, loss = 0.5561378598213196\n",
      "epoch: 6780, loss = 0.5558357834815979\n",
      "epoch: 6800, loss = 0.5555341243743896\n",
      "epoch: 6820, loss = 0.5552330613136292\n",
      "epoch: 6840, loss = 0.5549324154853821\n",
      "epoch: 6860, loss = 0.5546323657035828\n",
      "epoch: 6880, loss = 0.5543327331542969\n",
      "epoch: 6900, loss = 0.5540335774421692\n",
      "epoch: 6920, loss = 0.553735077381134\n",
      "epoch: 6940, loss = 0.5534369349479675\n",
      "epoch: 6960, loss = 0.5531393885612488\n",
      "epoch: 6980, loss = 0.5528423190116882\n",
      "epoch: 7000, loss = 0.5525457859039307\n",
      "epoch: 7020, loss = 0.5522497296333313\n",
      "epoch: 7040, loss = 0.5519540905952454\n",
      "epoch: 7060, loss = 0.551659107208252\n",
      "epoch: 7080, loss = 0.5513644218444824\n",
      "epoch: 7100, loss = 0.5510703921318054\n",
      "epoch: 7120, loss = 0.5507768988609314\n",
      "epoch: 7140, loss = 0.550483763217926\n",
      "epoch: 7160, loss = 0.5501912236213684\n",
      "epoch: 7180, loss = 0.5498991012573242\n",
      "epoch: 7200, loss = 0.549607515335083\n",
      "epoch: 7220, loss = 0.5493164658546448\n",
      "epoch: 7240, loss = 0.54902583360672\n",
      "epoch: 7260, loss = 0.5487357974052429\n",
      "epoch: 7280, loss = 0.5484461188316345\n",
      "epoch: 7300, loss = 0.5481569766998291\n",
      "epoch: 7320, loss = 0.5478683710098267\n",
      "epoch: 7340, loss = 0.5475802421569824\n",
      "epoch: 7360, loss = 0.5472925305366516\n",
      "epoch: 7380, loss = 0.5470054149627686\n",
      "epoch: 7400, loss = 0.5467187166213989\n",
      "epoch: 7420, loss = 0.5464325547218323\n",
      "epoch: 7440, loss = 0.5461468696594238\n",
      "epoch: 7460, loss = 0.5458616614341736\n",
      "epoch: 7480, loss = 0.5455768704414368\n",
      "epoch: 7500, loss = 0.5452926754951477\n",
      "epoch: 7520, loss = 0.5450088381767273\n",
      "epoch: 7540, loss = 0.5447255969047546\n",
      "epoch: 7560, loss = 0.5444428324699402\n",
      "epoch: 7580, loss = 0.5441604852676392\n",
      "epoch: 7600, loss = 0.5438786745071411\n",
      "epoch: 7620, loss = 0.5435972809791565\n",
      "epoch: 7640, loss = 0.5433164238929749\n",
      "epoch: 7660, loss = 0.5430360436439514\n",
      "epoch: 7680, loss = 0.5427560806274414\n",
      "epoch: 7700, loss = 0.5424766540527344\n",
      "epoch: 7720, loss = 0.5421977043151855\n",
      "epoch: 7740, loss = 0.5419192314147949\n",
      "epoch: 7760, loss = 0.5416412353515625\n",
      "epoch: 7780, loss = 0.5413637161254883\n",
      "epoch: 7800, loss = 0.5410865545272827\n",
      "epoch: 7820, loss = 0.5408100485801697\n",
      "epoch: 7840, loss = 0.5405339598655701\n",
      "epoch: 7860, loss = 0.5402583479881287\n",
      "epoch: 7880, loss = 0.5399832129478455\n",
      "epoch: 7900, loss = 0.5397084355354309\n",
      "epoch: 7920, loss = 0.5394342541694641\n",
      "epoch: 7940, loss = 0.5391604900360107\n",
      "epoch: 7960, loss = 0.5388872623443604\n",
      "epoch: 7980, loss = 0.5386143922805786\n",
      "epoch: 8000, loss = 0.5383420586585999\n",
      "epoch: 8020, loss = 0.5380702018737793\n",
      "epoch: 8040, loss = 0.5377987623214722\n",
      "epoch: 8060, loss = 0.537527859210968\n",
      "epoch: 8080, loss = 0.5372573733329773\n",
      "epoch: 8100, loss = 0.5369873642921448\n",
      "epoch: 8120, loss = 0.5367178320884705\n",
      "epoch: 8140, loss = 0.5364487767219543\n",
      "epoch: 8160, loss = 0.5361801385879517\n",
      "epoch: 8180, loss = 0.5359119772911072\n",
      "epoch: 8200, loss = 0.5356443524360657\n",
      "epoch: 8220, loss = 0.535377025604248\n",
      "epoch: 8240, loss = 0.5351104140281677\n",
      "epoch: 8260, loss = 0.5348441004753113\n",
      "epoch: 8280, loss = 0.534578263759613\n",
      "epoch: 8300, loss = 0.5343128442764282\n",
      "epoch: 8320, loss = 0.5340479016304016\n",
      "epoch: 8340, loss = 0.533783495426178\n",
      "epoch: 8360, loss = 0.533519446849823\n",
      "epoch: 8380, loss = 0.533255934715271\n",
      "epoch: 8400, loss = 0.5329928398132324\n",
      "epoch: 8420, loss = 0.532730221748352\n",
      "epoch: 8440, loss = 0.5324680209159851\n",
      "epoch: 8460, loss = 0.5322063565254211\n",
      "epoch: 8480, loss = 0.5319450497627258\n",
      "epoch: 8500, loss = 0.5316842198371887\n",
      "epoch: 8520, loss = 0.5314239859580994\n",
      "epoch: 8540, loss = 0.5311639904975891\n",
      "epoch: 8560, loss = 0.5309045910835266\n",
      "epoch: 8580, loss = 0.5306455492973328\n",
      "epoch: 8600, loss = 0.5303870439529419\n",
      "epoch: 8620, loss = 0.5301288962364197\n",
      "epoch: 8640, loss = 0.5298712253570557\n",
      "epoch: 8660, loss = 0.5296140313148499\n",
      "epoch: 8680, loss = 0.5293573141098022\n",
      "epoch: 8700, loss = 0.5291010737419128\n",
      "epoch: 8720, loss = 0.5288452506065369\n",
      "epoch: 8740, loss = 0.5285897850990295\n",
      "epoch: 8760, loss = 0.5283348560333252\n",
      "epoch: 8780, loss = 0.5280802845954895\n",
      "epoch: 8800, loss = 0.5278263092041016\n",
      "epoch: 8820, loss = 0.5275726318359375\n",
      "epoch: 8840, loss = 0.5273194313049316\n",
      "epoch: 8860, loss = 0.527066707611084\n",
      "epoch: 8880, loss = 0.5268144607543945\n",
      "epoch: 8900, loss = 0.5265626311302185\n",
      "epoch: 8920, loss = 0.5263112783432007\n",
      "epoch: 8940, loss = 0.5260602831840515\n",
      "epoch: 8960, loss = 0.5258097648620605\n",
      "epoch: 8980, loss = 0.5255597233772278\n",
      "epoch: 9000, loss = 0.5253100395202637\n",
      "epoch: 9020, loss = 0.5250608325004578\n",
      "epoch: 9040, loss = 0.5248121023178101\n",
      "epoch: 9060, loss = 0.5245637893676758\n",
      "epoch: 9080, loss = 0.5243159532546997\n",
      "epoch: 9100, loss = 0.5240684747695923\n",
      "epoch: 9120, loss = 0.5238214731216431\n",
      "epoch: 9140, loss = 0.5235748887062073\n",
      "epoch: 9160, loss = 0.5233288407325745\n",
      "epoch: 9180, loss = 0.5230830907821655\n",
      "epoch: 9200, loss = 0.5228378772735596\n",
      "epoch: 9220, loss = 0.5225930213928223\n",
      "epoch: 9240, loss = 0.5223485827445984\n",
      "epoch: 9260, loss = 0.5221046209335327\n",
      "epoch: 9280, loss = 0.5218610763549805\n",
      "epoch: 9300, loss = 0.5216180682182312\n",
      "epoch: 9320, loss = 0.521375298500061\n",
      "epoch: 9340, loss = 0.5211330652236938\n",
      "epoch: 9360, loss = 0.5208912491798401\n",
      "epoch: 9380, loss = 0.5206498503684998\n",
      "epoch: 9400, loss = 0.5204089283943176\n",
      "epoch: 9420, loss = 0.5201684832572937\n",
      "epoch: 9440, loss = 0.5199283361434937\n",
      "epoch: 9460, loss = 0.519688606262207\n",
      "epoch: 9480, loss = 0.5194494128227234\n",
      "epoch: 9500, loss = 0.5192105770111084\n",
      "epoch: 9520, loss = 0.5189721584320068\n",
      "epoch: 9540, loss = 0.5187342166900635\n",
      "epoch: 9560, loss = 0.5184966921806335\n",
      "epoch: 9580, loss = 0.5182595252990723\n",
      "epoch: 9600, loss = 0.518022894859314\n",
      "epoch: 9620, loss = 0.5177865028381348\n",
      "epoch: 9640, loss = 0.5175507068634033\n",
      "epoch: 9660, loss = 0.5173152685165405\n",
      "epoch: 9680, loss = 0.5170802474021912\n",
      "epoch: 9700, loss = 0.516845703125\n",
      "epoch: 9720, loss = 0.5166114568710327\n",
      "epoch: 9740, loss = 0.5163777470588684\n",
      "epoch: 9760, loss = 0.5161444544792175\n",
      "epoch: 9780, loss = 0.5159114599227905\n",
      "epoch: 9800, loss = 0.5156789422035217\n",
      "epoch: 9820, loss = 0.5154468417167664\n",
      "epoch: 9840, loss = 0.5152152180671692\n",
      "epoch: 9860, loss = 0.5149839520454407\n",
      "epoch: 9880, loss = 0.5147531032562256\n",
      "epoch: 9900, loss = 0.5145227313041687\n",
      "epoch: 9920, loss = 0.5142927169799805\n",
      "epoch: 9940, loss = 0.5140630602836609\n",
      "epoch: 9960, loss = 0.5138338208198547\n",
      "epoch: 9980, loss = 0.5136051774024963\n",
      "epoch: 10000, loss = 0.5133766531944275\n",
      "epoch: 10020, loss = 0.5131487846374512\n",
      "epoch: 10040, loss = 0.5129212141036987\n",
      "epoch: 10060, loss = 0.5126940608024597\n",
      "epoch: 10080, loss = 0.5124673247337341\n",
      "epoch: 10100, loss = 0.512241005897522\n",
      "epoch: 10120, loss = 0.5120151042938232\n",
      "epoch: 10140, loss = 0.5117895603179932\n",
      "epoch: 10160, loss = 0.5115644335746765\n",
      "epoch: 10180, loss = 0.5113397240638733\n",
      "epoch: 10200, loss = 0.5111154913902283\n",
      "epoch: 10220, loss = 0.5108915567398071\n",
      "epoch: 10240, loss = 0.510668158531189\n",
      "epoch: 10260, loss = 0.5104449987411499\n",
      "epoch: 10280, loss = 0.510222315788269\n",
      "epoch: 10300, loss = 0.5100000500679016\n",
      "epoch: 10320, loss = 0.5097782015800476\n",
      "epoch: 10340, loss = 0.5095567107200623\n",
      "epoch: 10360, loss = 0.5093356370925903\n",
      "epoch: 10380, loss = 0.5091149806976318\n",
      "epoch: 10400, loss = 0.508894681930542\n",
      "epoch: 10420, loss = 0.5086748003959656\n",
      "epoch: 10440, loss = 0.5084553360939026\n",
      "epoch: 10460, loss = 0.5082361698150635\n",
      "epoch: 10480, loss = 0.5080175399780273\n",
      "epoch: 10500, loss = 0.5077992081642151\n",
      "epoch: 10520, loss = 0.5075812935829163\n",
      "epoch: 10540, loss = 0.5073637962341309\n",
      "epoch: 10560, loss = 0.5071467161178589\n",
      "epoch: 10580, loss = 0.5069299936294556\n",
      "epoch: 10600, loss = 0.5067136883735657\n",
      "epoch: 10620, loss = 0.5064976811408997\n",
      "epoch: 10640, loss = 0.5062821507453918\n",
      "epoch: 10660, loss = 0.5060670971870422\n",
      "epoch: 10680, loss = 0.5058522820472717\n",
      "epoch: 10700, loss = 0.5056379437446594\n",
      "epoch: 10720, loss = 0.5054239630699158\n",
      "epoch: 10740, loss = 0.5052103400230408\n",
      "epoch: 10760, loss = 0.5049970746040344\n",
      "epoch: 10780, loss = 0.504784345626831\n",
      "epoch: 10800, loss = 0.5045719146728516\n",
      "epoch: 10820, loss = 0.5043598413467407\n",
      "epoch: 10840, loss = 0.5041481256484985\n",
      "epoch: 10860, loss = 0.5039368867874146\n",
      "epoch: 10880, loss = 0.5037260055541992\n",
      "epoch: 10900, loss = 0.5035154223442078\n",
      "epoch: 10920, loss = 0.5033053159713745\n",
      "epoch: 10940, loss = 0.5030955672264099\n",
      "epoch: 10960, loss = 0.5028862357139587\n",
      "epoch: 10980, loss = 0.5026772618293762\n",
      "epoch: 11000, loss = 0.5024686455726624\n",
      "epoch: 11020, loss = 0.5022604465484619\n",
      "epoch: 11040, loss = 0.5020525455474854\n",
      "epoch: 11060, loss = 0.5018450617790222\n",
      "epoch: 11080, loss = 0.5016379952430725\n",
      "epoch: 11100, loss = 0.5014313459396362\n",
      "epoch: 11120, loss = 0.5012249946594238\n",
      "epoch: 11140, loss = 0.5010190010070801\n",
      "epoch: 11160, loss = 0.500813364982605\n",
      "epoch: 11180, loss = 0.5006082057952881\n",
      "epoch: 11200, loss = 0.5004034042358398\n",
      "epoch: 11220, loss = 0.5001989006996155\n",
      "epoch: 11240, loss = 0.49999481439590454\n",
      "epoch: 11260, loss = 0.49979108572006226\n",
      "epoch: 11280, loss = 0.4995877742767334\n",
      "epoch: 11300, loss = 0.4993847906589508\n",
      "epoch: 11320, loss = 0.4991820752620697\n",
      "epoch: 11340, loss = 0.4989798665046692\n",
      "epoch: 11360, loss = 0.49877798557281494\n",
      "epoch: 11380, loss = 0.4985765218734741\n",
      "epoch: 11400, loss = 0.49837538599967957\n",
      "epoch: 11420, loss = 0.4981745779514313\n",
      "epoch: 11440, loss = 0.497974157333374\n",
      "epoch: 11460, loss = 0.497774213552475\n",
      "epoch: 11480, loss = 0.4975745379924774\n",
      "epoch: 11500, loss = 0.4973751902580261\n",
      "epoch: 11520, loss = 0.49717625975608826\n",
      "epoch: 11540, loss = 0.49697768688201904\n",
      "epoch: 11560, loss = 0.4967794716358185\n",
      "epoch: 11580, loss = 0.4965815842151642\n",
      "epoch: 11600, loss = 0.4963842034339905\n",
      "epoch: 11620, loss = 0.4961869716644287\n",
      "epoch: 11640, loss = 0.49599021673202515\n",
      "epoch: 11660, loss = 0.49579375982284546\n",
      "epoch: 11680, loss = 0.4955977201461792\n",
      "epoch: 11700, loss = 0.4954020082950592\n",
      "epoch: 11720, loss = 0.49520665407180786\n",
      "epoch: 11740, loss = 0.49501174688339233\n",
      "epoch: 11760, loss = 0.4948171079158783\n",
      "epoch: 11780, loss = 0.4946228563785553\n",
      "epoch: 11800, loss = 0.4944289028644562\n",
      "epoch: 11820, loss = 0.4942353069782257\n",
      "epoch: 11840, loss = 0.49404218792915344\n",
      "epoch: 11860, loss = 0.4938492476940155\n",
      "epoch: 11880, loss = 0.49365678429603577\n",
      "epoch: 11900, loss = 0.4934646487236023\n",
      "epoch: 11920, loss = 0.4932728111743927\n",
      "epoch: 11940, loss = 0.4930814206600189\n",
      "epoch: 11960, loss = 0.49289023876190186\n",
      "epoch: 11980, loss = 0.4926995635032654\n",
      "epoch: 12000, loss = 0.492509126663208\n",
      "epoch: 12020, loss = 0.4923190176486969\n",
      "epoch: 12040, loss = 0.4921293556690216\n",
      "epoch: 12060, loss = 0.4919400215148926\n",
      "epoch: 12080, loss = 0.4917509853839874\n",
      "epoch: 12100, loss = 0.4915623366832733\n",
      "epoch: 12120, loss = 0.4913739860057831\n",
      "epoch: 12140, loss = 0.4911859929561615\n",
      "epoch: 12160, loss = 0.49099838733673096\n",
      "epoch: 12180, loss = 0.4908110797405243\n",
      "epoch: 12200, loss = 0.4906240999698639\n",
      "epoch: 12220, loss = 0.4904375374317169\n",
      "epoch: 12240, loss = 0.4902512729167938\n",
      "epoch: 12260, loss = 0.490065336227417\n",
      "epoch: 12280, loss = 0.48987969756126404\n",
      "epoch: 12300, loss = 0.4896944761276245\n",
      "epoch: 12320, loss = 0.48950955271720886\n",
      "epoch: 12340, loss = 0.48932498693466187\n",
      "epoch: 12360, loss = 0.48914071917533875\n",
      "epoch: 12380, loss = 0.48895686864852905\n",
      "epoch: 12400, loss = 0.48877325654029846\n",
      "epoch: 12420, loss = 0.4885900616645813\n",
      "epoch: 12440, loss = 0.4884071350097656\n",
      "epoch: 12460, loss = 0.4882245659828186\n",
      "epoch: 12480, loss = 0.48804229497909546\n",
      "epoch: 12500, loss = 0.48786047101020813\n",
      "epoch: 12520, loss = 0.4876788556575775\n",
      "epoch: 12540, loss = 0.48749762773513794\n",
      "epoch: 12560, loss = 0.487316757440567\n",
      "epoch: 12580, loss = 0.48713621497154236\n",
      "epoch: 12600, loss = 0.4869559109210968\n",
      "epoch: 12620, loss = 0.4867760241031647\n",
      "epoch: 12640, loss = 0.4865964353084564\n",
      "epoch: 12660, loss = 0.4864172041416168\n",
      "epoch: 12680, loss = 0.4862382113933563\n",
      "epoch: 12700, loss = 0.48605963587760925\n",
      "epoch: 12720, loss = 0.48588135838508606\n",
      "epoch: 12740, loss = 0.48570340871810913\n",
      "epoch: 12760, loss = 0.48552581667900085\n",
      "epoch: 12780, loss = 0.4853484630584717\n",
      "epoch: 12800, loss = 0.48517149686813354\n",
      "epoch: 12820, loss = 0.4849947988986969\n",
      "epoch: 12840, loss = 0.4848184287548065\n",
      "epoch: 12860, loss = 0.4846424460411072\n",
      "epoch: 12880, loss = 0.4844667911529541\n",
      "epoch: 12900, loss = 0.4842914044857025\n",
      "epoch: 12920, loss = 0.4841163456439972\n",
      "epoch: 12940, loss = 0.4839416444301605\n",
      "epoch: 12960, loss = 0.48376715183258057\n",
      "epoch: 12980, loss = 0.48359307646751404\n",
      "epoch: 13000, loss = 0.4834192991256714\n",
      "epoch: 13020, loss = 0.483245849609375\n",
      "epoch: 13040, loss = 0.4830726683139801\n",
      "epoch: 13060, loss = 0.48289984464645386\n",
      "epoch: 13080, loss = 0.4827273190021515\n",
      "epoch: 13100, loss = 0.4825550615787506\n",
      "epoch: 13120, loss = 0.48238319158554077\n",
      "epoch: 13140, loss = 0.4822116494178772\n",
      "epoch: 13160, loss = 0.4820403754711151\n",
      "epoch: 13180, loss = 0.48186933994293213\n",
      "epoch: 13200, loss = 0.4816986918449402\n",
      "epoch: 13220, loss = 0.4815283715724945\n",
      "epoch: 13240, loss = 0.4813583493232727\n",
      "epoch: 13260, loss = 0.48118868470191956\n",
      "epoch: 13280, loss = 0.4810192286968231\n",
      "epoch: 13300, loss = 0.48085013031959534\n",
      "epoch: 13320, loss = 0.48068132996559143\n",
      "epoch: 13340, loss = 0.480512797832489\n",
      "epoch: 13360, loss = 0.48034465312957764\n",
      "epoch: 13380, loss = 0.48017677664756775\n",
      "epoch: 13400, loss = 0.48000919818878174\n",
      "epoch: 13420, loss = 0.4798419177532196\n",
      "epoch: 13440, loss = 0.4796749949455261\n",
      "epoch: 13460, loss = 0.47950831055641174\n",
      "epoch: 13480, loss = 0.47934195399284363\n",
      "epoch: 13500, loss = 0.4791758358478546\n",
      "epoch: 13520, loss = 0.47901007533073425\n",
      "epoch: 13540, loss = 0.47884467244148254\n",
      "epoch: 13560, loss = 0.47867950797080994\n",
      "epoch: 13580, loss = 0.4785146117210388\n",
      "epoch: 13600, loss = 0.47835007309913635\n",
      "epoch: 13620, loss = 0.4781858026981354\n",
      "epoch: 13640, loss = 0.47802186012268066\n",
      "epoch: 13660, loss = 0.47785818576812744\n",
      "epoch: 13680, loss = 0.4776947498321533\n",
      "epoch: 13700, loss = 0.47753170132637024\n",
      "epoch: 13720, loss = 0.47736889123916626\n",
      "epoch: 13740, loss = 0.47720640897750854\n",
      "epoch: 13760, loss = 0.4770442247390747\n",
      "epoch: 13780, loss = 0.47688230872154236\n",
      "epoch: 13800, loss = 0.4767206907272339\n",
      "epoch: 13820, loss = 0.4765594005584717\n",
      "epoch: 13840, loss = 0.4763983488082886\n",
      "epoch: 13860, loss = 0.47623762488365173\n",
      "epoch: 13880, loss = 0.476077139377594\n",
      "epoch: 13900, loss = 0.4759170413017273\n",
      "epoch: 13920, loss = 0.4757571220397949\n",
      "epoch: 13940, loss = 0.4755975604057312\n",
      "epoch: 13960, loss = 0.4754382073879242\n",
      "epoch: 13980, loss = 0.47527918219566345\n",
      "epoch: 14000, loss = 0.47512051463127136\n",
      "epoch: 14020, loss = 0.47496211528778076\n",
      "epoch: 14040, loss = 0.4748039245605469\n",
      "epoch: 14060, loss = 0.47464603185653687\n",
      "epoch: 14080, loss = 0.4744884967803955\n",
      "epoch: 14100, loss = 0.47433117032051086\n",
      "epoch: 14120, loss = 0.4741741716861725\n",
      "epoch: 14140, loss = 0.4740174114704132\n",
      "epoch: 14160, loss = 0.4738609790802002\n",
      "epoch: 14180, loss = 0.47370481491088867\n",
      "epoch: 14200, loss = 0.4735489785671234\n",
      "epoch: 14220, loss = 0.47339335083961487\n",
      "epoch: 14240, loss = 0.4732380211353302\n",
      "epoch: 14260, loss = 0.473082959651947\n",
      "epoch: 14280, loss = 0.47292816638946533\n",
      "epoch: 14300, loss = 0.47277364134788513\n",
      "epoch: 14320, loss = 0.4726194739341736\n",
      "epoch: 14340, loss = 0.47246554493904114\n",
      "epoch: 14360, loss = 0.4723118841648102\n",
      "epoch: 14380, loss = 0.4721585214138031\n",
      "epoch: 14400, loss = 0.47200536727905273\n",
      "epoch: 14420, loss = 0.47185251116752625\n",
      "epoch: 14440, loss = 0.471699982881546\n",
      "epoch: 14460, loss = 0.4715476930141449\n",
      "epoch: 14480, loss = 0.47139567136764526\n",
      "epoch: 14500, loss = 0.4712439179420471\n",
      "epoch: 14520, loss = 0.47109246253967285\n",
      "epoch: 14540, loss = 0.4709412455558777\n",
      "epoch: 14560, loss = 0.470790296792984\n",
      "epoch: 14580, loss = 0.4706396162509918\n",
      "epoch: 14600, loss = 0.4704892337322235\n",
      "epoch: 14620, loss = 0.4703391194343567\n",
      "epoch: 14640, loss = 0.47018927335739136\n",
      "epoch: 14660, loss = 0.47003963589668274\n",
      "epoch: 14680, loss = 0.4698903262615204\n",
      "epoch: 14700, loss = 0.46974125504493713\n",
      "epoch: 14720, loss = 0.46959248185157776\n",
      "epoch: 14740, loss = 0.4694439470767975\n",
      "epoch: 14760, loss = 0.4692956805229187\n",
      "epoch: 14780, loss = 0.46914762258529663\n",
      "epoch: 14800, loss = 0.4689998924732208\n",
      "epoch: 14820, loss = 0.4688524305820465\n",
      "epoch: 14840, loss = 0.46870526671409607\n",
      "epoch: 14860, loss = 0.46855828166007996\n",
      "epoch: 14880, loss = 0.4684116542339325\n",
      "epoch: 14900, loss = 0.4682651460170746\n",
      "epoch: 14920, loss = 0.46811896562576294\n",
      "epoch: 14940, loss = 0.46797308325767517\n",
      "epoch: 14960, loss = 0.4678274691104889\n",
      "epoch: 14980, loss = 0.4676820635795593\n",
      "epoch: 15000, loss = 0.46753695607185364\n",
      "epoch: 15020, loss = 0.46739208698272705\n",
      "epoch: 15040, loss = 0.46724748611450195\n",
      "epoch: 15060, loss = 0.4671030640602112\n",
      "epoch: 15080, loss = 0.4669589698314667\n",
      "epoch: 15100, loss = 0.46681511402130127\n",
      "epoch: 15120, loss = 0.46667155623435974\n",
      "epoch: 15140, loss = 0.4665282070636749\n",
      "epoch: 15160, loss = 0.46638503670692444\n",
      "epoch: 15180, loss = 0.466242253780365\n",
      "epoch: 15200, loss = 0.46609964966773987\n",
      "epoch: 15220, loss = 0.46595731377601624\n",
      "epoch: 15240, loss = 0.4658152163028717\n",
      "epoch: 15260, loss = 0.4656733274459839\n",
      "epoch: 15280, loss = 0.4655318260192871\n",
      "epoch: 15300, loss = 0.46539047360420227\n",
      "epoch: 15320, loss = 0.4652494192123413\n",
      "epoch: 15340, loss = 0.46510857343673706\n",
      "epoch: 15360, loss = 0.4649679660797119\n",
      "epoch: 15380, loss = 0.46482765674591064\n",
      "epoch: 15400, loss = 0.4646875262260437\n",
      "epoch: 15420, loss = 0.46454769372940063\n",
      "epoch: 15440, loss = 0.46440809965133667\n",
      "epoch: 15460, loss = 0.4642687737941742\n",
      "epoch: 15480, loss = 0.46412965655326843\n",
      "epoch: 15500, loss = 0.46399083733558655\n",
      "epoch: 15520, loss = 0.463852196931839\n",
      "epoch: 15540, loss = 0.46371376514434814\n",
      "epoch: 15560, loss = 0.4635756313800812\n",
      "epoch: 15580, loss = 0.4634377956390381\n",
      "epoch: 15600, loss = 0.4633001387119293\n",
      "epoch: 15620, loss = 0.46316275000572205\n",
      "epoch: 15640, loss = 0.4630255103111267\n",
      "epoch: 15660, loss = 0.4628886282444\n",
      "epoch: 15680, loss = 0.46275195479393005\n",
      "epoch: 15700, loss = 0.4626154899597168\n",
      "epoch: 15720, loss = 0.46247926354408264\n",
      "epoch: 15740, loss = 0.46234333515167236\n",
      "epoch: 15760, loss = 0.462207555770874\n",
      "epoch: 15780, loss = 0.4620719850063324\n",
      "epoch: 15800, loss = 0.4619368016719818\n",
      "epoch: 15820, loss = 0.46180182695388794\n",
      "epoch: 15840, loss = 0.4616670310497284\n",
      "epoch: 15860, loss = 0.46153244376182556\n",
      "epoch: 15880, loss = 0.46139809489250183\n",
      "epoch: 15900, loss = 0.461264044046402\n",
      "epoch: 15920, loss = 0.4611302316188812\n",
      "epoch: 15940, loss = 0.4609966278076172\n",
      "epoch: 15960, loss = 0.4608632028102875\n",
      "epoch: 15980, loss = 0.46073004603385925\n",
      "epoch: 16000, loss = 0.46059712767601013\n",
      "epoch: 16020, loss = 0.4604644477367401\n",
      "epoch: 16040, loss = 0.4603319466114044\n",
      "epoch: 16060, loss = 0.4601997435092926\n",
      "epoch: 16080, loss = 0.4600677490234375\n",
      "epoch: 16100, loss = 0.45993590354919434\n",
      "epoch: 16120, loss = 0.4598044157028198\n",
      "epoch: 16140, loss = 0.45967304706573486\n",
      "epoch: 16160, loss = 0.45954200625419617\n",
      "epoch: 16180, loss = 0.4594111740589142\n",
      "epoch: 16200, loss = 0.45928049087524414\n",
      "epoch: 16220, loss = 0.4591500759124756\n",
      "epoch: 16240, loss = 0.4590199291706085\n",
      "epoch: 16260, loss = 0.45888999104499817\n",
      "epoch: 16280, loss = 0.45876020193099976\n",
      "epoch: 16300, loss = 0.45863068103790283\n",
      "epoch: 16320, loss = 0.458501398563385\n",
      "epoch: 16340, loss = 0.4583723545074463\n",
      "epoch: 16360, loss = 0.4582434892654419\n",
      "epoch: 16380, loss = 0.458114892244339\n",
      "epoch: 16400, loss = 0.4579865336418152\n",
      "epoch: 16420, loss = 0.45785829424858093\n",
      "epoch: 16440, loss = 0.45773041248321533\n",
      "epoch: 16460, loss = 0.45760267972946167\n",
      "epoch: 16480, loss = 0.4574751853942871\n",
      "epoch: 16500, loss = 0.4573478400707245\n",
      "epoch: 16520, loss = 0.45722076296806335\n",
      "epoch: 16540, loss = 0.4570939242839813\n",
      "epoch: 16560, loss = 0.4569672644138336\n",
      "epoch: 16580, loss = 0.4568408131599426\n",
      "epoch: 16600, loss = 0.4567146301269531\n",
      "epoch: 16620, loss = 0.45658859610557556\n",
      "epoch: 16640, loss = 0.4564628601074219\n",
      "epoch: 16660, loss = 0.4563373029232025\n",
      "epoch: 16680, loss = 0.45621198415756226\n",
      "epoch: 16700, loss = 0.45608681440353394\n",
      "epoch: 16720, loss = 0.45596185326576233\n",
      "epoch: 16740, loss = 0.4558371603488922\n",
      "epoch: 16760, loss = 0.4557127356529236\n",
      "epoch: 16780, loss = 0.4555884599685669\n",
      "epoch: 16800, loss = 0.4554644227027893\n",
      "epoch: 16820, loss = 0.4553404748439789\n",
      "epoch: 16840, loss = 0.4552169144153595\n",
      "epoch: 16860, loss = 0.45509347319602966\n",
      "epoch: 16880, loss = 0.45497027039527893\n",
      "epoch: 16900, loss = 0.45484718680381775\n",
      "epoch: 16920, loss = 0.45472440123558044\n",
      "epoch: 16940, loss = 0.45460182428359985\n",
      "epoch: 16960, loss = 0.454479455947876\n",
      "epoch: 16980, loss = 0.4543572664260864\n",
      "epoch: 17000, loss = 0.4542352557182312\n",
      "epoch: 17020, loss = 0.45411354303359985\n",
      "epoch: 17040, loss = 0.45399194955825806\n",
      "epoch: 17060, loss = 0.45387059450149536\n",
      "epoch: 17080, loss = 0.45374947786331177\n",
      "epoch: 17100, loss = 0.4536285698413849\n",
      "epoch: 17120, loss = 0.45350778102874756\n",
      "epoch: 17140, loss = 0.4533872902393341\n",
      "epoch: 17160, loss = 0.453266978263855\n",
      "epoch: 17180, loss = 0.45314687490463257\n",
      "epoch: 17200, loss = 0.4530269503593445\n",
      "epoch: 17220, loss = 0.45290717482566833\n",
      "epoch: 17240, loss = 0.4527876675128937\n",
      "epoch: 17260, loss = 0.4526683986186981\n",
      "epoch: 17280, loss = 0.4525492787361145\n",
      "epoch: 17300, loss = 0.4524304270744324\n",
      "epoch: 17320, loss = 0.4523116946220398\n",
      "epoch: 17340, loss = 0.45219314098358154\n",
      "epoch: 17360, loss = 0.45207479596138\n",
      "epoch: 17380, loss = 0.45195671916007996\n",
      "epoch: 17400, loss = 0.45183882117271423\n",
      "epoch: 17420, loss = 0.45172107219696045\n",
      "epoch: 17440, loss = 0.45160356163978577\n",
      "epoch: 17460, loss = 0.4514862596988678\n",
      "epoch: 17480, loss = 0.45136910676956177\n",
      "epoch: 17500, loss = 0.4512522220611572\n",
      "epoch: 17520, loss = 0.4511354863643646\n",
      "epoch: 17540, loss = 0.4510189890861511\n",
      "epoch: 17560, loss = 0.4509025514125824\n",
      "epoch: 17580, loss = 0.45078644156455994\n",
      "epoch: 17600, loss = 0.4506705105304718\n",
      "epoch: 17620, loss = 0.4505547881126404\n",
      "epoch: 17640, loss = 0.4504391849040985\n",
      "epoch: 17660, loss = 0.45032384991645813\n",
      "epoch: 17680, loss = 0.4502086341381073\n",
      "epoch: 17700, loss = 0.45009368658065796\n",
      "epoch: 17720, loss = 0.44997888803482056\n",
      "epoch: 17740, loss = 0.4498642683029175\n",
      "epoch: 17760, loss = 0.4497498869895935\n",
      "epoch: 17780, loss = 0.44963568449020386\n",
      "epoch: 17800, loss = 0.44952160120010376\n",
      "epoch: 17820, loss = 0.44940775632858276\n",
      "epoch: 17840, loss = 0.4492941200733185\n",
      "epoch: 17860, loss = 0.4491806924343109\n",
      "epoch: 17880, loss = 0.4490673840045929\n",
      "epoch: 17900, loss = 0.448954313993454\n",
      "epoch: 17920, loss = 0.448841392993927\n",
      "epoch: 17940, loss = 0.44872868061065674\n",
      "epoch: 17960, loss = 0.4486162066459656\n",
      "epoch: 17980, loss = 0.4485038220882416\n",
      "epoch: 18000, loss = 0.44839170575141907\n",
      "epoch: 18020, loss = 0.4482797384262085\n",
      "epoch: 18040, loss = 0.44816797971725464\n",
      "epoch: 18060, loss = 0.4480563998222351\n",
      "epoch: 18080, loss = 0.4479449391365051\n",
      "epoch: 18100, loss = 0.44783371686935425\n",
      "epoch: 18120, loss = 0.4477226436138153\n",
      "epoch: 18140, loss = 0.44761183857917786\n",
      "epoch: 18160, loss = 0.44750112295150757\n",
      "epoch: 18180, loss = 0.4473906457424164\n",
      "epoch: 18200, loss = 0.4472803473472595\n",
      "epoch: 18220, loss = 0.447170227766037\n",
      "epoch: 18240, loss = 0.4470602571964264\n",
      "epoch: 18260, loss = 0.4469504654407501\n",
      "epoch: 18280, loss = 0.44684088230133057\n",
      "epoch: 18300, loss = 0.44673147797584534\n",
      "epoch: 18320, loss = 0.4466222822666168\n",
      "epoch: 18340, loss = 0.44651320576667786\n",
      "epoch: 18360, loss = 0.446404367685318\n",
      "epoch: 18380, loss = 0.4462956488132477\n",
      "epoch: 18400, loss = 0.4461870789527893\n",
      "epoch: 18420, loss = 0.4460787773132324\n",
      "epoch: 18440, loss = 0.44597068428993225\n",
      "epoch: 18460, loss = 0.44586265087127686\n",
      "epoch: 18480, loss = 0.44575485587120056\n",
      "epoch: 18500, loss = 0.4456471800804138\n",
      "epoch: 18520, loss = 0.44553977251052856\n",
      "epoch: 18540, loss = 0.44543248414993286\n",
      "epoch: 18560, loss = 0.44532540440559387\n",
      "epoch: 18580, loss = 0.44521844387054443\n",
      "epoch: 18600, loss = 0.4451117515563965\n",
      "epoch: 18620, loss = 0.4450051486492157\n",
      "epoch: 18640, loss = 0.4448987543582916\n",
      "epoch: 18660, loss = 0.4447925090789795\n",
      "epoch: 18680, loss = 0.4446864724159241\n",
      "epoch: 18700, loss = 0.444580614566803\n",
      "epoch: 18720, loss = 0.44447484612464905\n",
      "epoch: 18740, loss = 0.44436928629875183\n",
      "epoch: 18760, loss = 0.44426393508911133\n",
      "epoch: 18780, loss = 0.44415873289108276\n",
      "epoch: 18800, loss = 0.44405367970466614\n",
      "epoch: 18820, loss = 0.44394877552986145\n",
      "epoch: 18840, loss = 0.44384413957595825\n",
      "epoch: 18860, loss = 0.44373956322669983\n",
      "epoch: 18880, loss = 0.4436352252960205\n",
      "epoch: 18900, loss = 0.4435310661792755\n",
      "epoch: 18920, loss = 0.44342705607414246\n",
      "epoch: 18940, loss = 0.4433232247829437\n",
      "epoch: 18960, loss = 0.44321951270103455\n",
      "epoch: 18980, loss = 0.4431159496307373\n",
      "epoch: 19000, loss = 0.44301265478134155\n",
      "epoch: 19020, loss = 0.44290947914123535\n",
      "epoch: 19040, loss = 0.4428064227104187\n",
      "epoch: 19060, loss = 0.44270363450050354\n",
      "epoch: 19080, loss = 0.44260093569755554\n",
      "epoch: 19100, loss = 0.4424983859062195\n",
      "epoch: 19120, loss = 0.44239604473114014\n",
      "epoch: 19140, loss = 0.44229385256767273\n",
      "epoch: 19160, loss = 0.44219180941581726\n",
      "epoch: 19180, loss = 0.4420899748802185\n",
      "epoch: 19200, loss = 0.4419882297515869\n",
      "epoch: 19220, loss = 0.4418867230415344\n",
      "epoch: 19240, loss = 0.4417852759361267\n",
      "epoch: 19260, loss = 0.44168415665626526\n",
      "epoch: 19280, loss = 0.4415830969810486\n",
      "epoch: 19300, loss = 0.44148221611976624\n",
      "epoch: 19320, loss = 0.44138142466545105\n",
      "epoch: 19340, loss = 0.44128090143203735\n",
      "epoch: 19360, loss = 0.4411804676055908\n",
      "epoch: 19380, loss = 0.441080242395401\n",
      "epoch: 19400, loss = 0.4409801959991455\n",
      "epoch: 19420, loss = 0.4408802092075348\n",
      "epoch: 19440, loss = 0.4407804608345032\n",
      "epoch: 19460, loss = 0.4406808912754059\n",
      "epoch: 19480, loss = 0.44058138132095337\n",
      "epoch: 19500, loss = 0.44048207998275757\n",
      "epoch: 19520, loss = 0.44038301706314087\n",
      "epoch: 19540, loss = 0.44028404355049133\n",
      "epoch: 19560, loss = 0.44018515944480896\n",
      "epoch: 19580, loss = 0.4400865435600281\n",
      "epoch: 19600, loss = 0.43998801708221436\n",
      "epoch: 19620, loss = 0.43988966941833496\n",
      "epoch: 19640, loss = 0.4397915005683899\n",
      "epoch: 19660, loss = 0.439693421125412\n",
      "epoch: 19680, loss = 0.4395955801010132\n",
      "epoch: 19700, loss = 0.43949782848358154\n",
      "epoch: 19720, loss = 0.43940025568008423\n",
      "epoch: 19740, loss = 0.4393027722835541\n",
      "epoch: 19760, loss = 0.4392055571079254\n",
      "epoch: 19780, loss = 0.4391084313392639\n",
      "epoch: 19800, loss = 0.43901148438453674\n",
      "epoch: 19820, loss = 0.4389146566390991\n",
      "epoch: 19840, loss = 0.4388180077075958\n",
      "epoch: 19860, loss = 0.43872153759002686\n",
      "epoch: 19880, loss = 0.43862515687942505\n",
      "epoch: 19900, loss = 0.43852895498275757\n",
      "epoch: 19920, loss = 0.43843287229537964\n",
      "epoch: 19940, loss = 0.43833693861961365\n",
      "epoch: 19960, loss = 0.43824124336242676\n",
      "epoch: 19980, loss = 0.4381456673145294\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "model.train()\n",
    "for epoch in range(20000):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train)\n",
    "    loss = loss_func(y_pred, y_train) \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"epoch: {epoch}, loss = {loss.item()}\")\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "+ Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9231\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test)\n",
    "    y_predicted_cls = logits.max(-1)[1]\n",
    "    acc = (y_predicted_cls == y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy: {acc.item():.4f}')\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.92        13\n",
      "   macro avg       0.88      0.95      0.90        13\n",
      "weighted avg       0.94      0.92      0.93        13\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2  Handwriting recognition with MLP\n",
    "\n",
    "Like last week's lab , your task in this section is also about recognizing handwritten digits, but you are required to use MLP to complete the exercise. It is recommended that you define an MLP class, which is a subclass of `nn.module`.\n",
    "\n",
    "<font color='red' size=4>Note that your accuracy in this section will directly determine your score.</font>\n",
    "\n",
    "For this exercise we use the `minist` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]),\n",
       " torch.Size([60000]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "\n",
    "X_train, Y_train = loadlocal_mnist(\"./datasets/MNIST/raw/train-images-idx3-ubyte\", \"./datasets/MNIST/raw/train-labels-idx1-ubyte\")\n",
    "X_test, Y_test = loadlocal_mnist(\"./datasets/MNIST/raw/t10k-images-idx3-ubyte\", \"./datasets/MNIST/raw/t10k-labels-idx1-ubyte\")\n",
    "X_train, Y_train, X_test, Y_test = torch.from_numpy(X_train).float(), torch.from_numpy(Y_train), torch.from_numpy(X_test).float(), torch.from_numpy(Y_test)\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n",
    "############################################                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Define a MLP subclass of nn. Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "from torch import nn\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, input):\n",
    "        return self.linear2(self.relu(self.linear1(input)))\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (linear1): Linear(in_features=784, out_features=392, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (linear2): Linear(in_features=392, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "model = MLP(784, 392, 10)\n",
    "model\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " + Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Write Your Code Here ###########\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1e-3)\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]. Loss: 28.29774284362793\n",
      "[1 101]. Loss: 0.6092483997344971\n",
      "[1 201]. Loss: 0.4006517231464386\n",
      "[1 301]. Loss: 0.30777794122695923\n",
      "[1 401]. Loss: 0.2521504759788513\n",
      "[1 501]. Loss: 0.2142932116985321\n",
      "[2 1]. Loss: 0.2139735221862793\n",
      "[2 101]. Loss: 0.18614083528518677\n",
      "[2 201]. Loss: 0.16461072862148285\n",
      "[2 301]. Loss: 0.14725157618522644\n",
      "[2 401]. Loss: 0.13288317620754242\n",
      "[2 501]. Loss: 0.12075548619031906\n",
      "[3 1]. Loss: 0.12064367532730103\n",
      "[3 101]. Loss: 0.11025107651948929\n",
      "[3 201]. Loss: 0.10123691707849503\n",
      "[3 301]. Loss: 0.09332927316427231\n",
      "[3 401]. Loss: 0.08634068071842194\n",
      "[3 501]. Loss: 0.08011677861213684\n",
      "[4 1]. Loss: 0.08005794137716293\n",
      "[4 101]. Loss: 0.07448475062847137\n",
      "[4 201]. Loss: 0.06947505474090576\n",
      "[4 301]. Loss: 0.06494085490703583\n",
      "[4 401]. Loss: 0.060819271951913834\n",
      "[4 501]. Loss: 0.05705341696739197\n",
      "[5 1]. Loss: 0.05701741576194763\n",
      "[5 101]. Loss: 0.05357162654399872\n",
      "[5 201]. Loss: 0.050403010100126266\n",
      "[5 301]. Loss: 0.04748457670211792\n",
      "[5 401]. Loss: 0.04479258507490158\n",
      "[5 501]. Loss: 0.04230007901787758\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########### Write Your Code Here ###########\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for e in range(501):\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X_train)\n",
    "        loss = loss_func(pred, Y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 100 == 0:\n",
    "            print(f\"[{epoch+1} {e+1}]. Loss: {loss.item()}\")\n",
    "############################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9518\n"
     ]
    }
   ],
   "source": [
    "########### Write Your Code Here ###########\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test)\n",
    "    y_predicted_cls = logits.max(-1)[1]\n",
    "    acc = (y_predicted_cls == Y_test).sum() / float(Y_test.shape[0])\n",
    "    print(f'accuracy: {acc.item():.4f}')\n",
    "############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97       980\n",
      "           1       0.98      0.98      0.98      1135\n",
      "           2       0.96      0.94      0.95      1032\n",
      "           3       0.94      0.94      0.94      1010\n",
      "           4       0.95      0.95      0.95       982\n",
      "           5       0.93      0.96      0.94       892\n",
      "           6       0.97      0.95      0.96       958\n",
      "           7       0.95      0.96      0.95      1028\n",
      "           8       0.94      0.93      0.93       974\n",
      "           9       0.94      0.93      0.93      1009\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.95      0.95      0.95     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(Y_test, y_predicted_cls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3  Questions (10 points )\n",
    "1.What's the difference between logistic regression and Perceptron?\n",
    "\n",
    "2.Advantages and disadvantages of neural networks?\n",
    "\n",
    "3.What is the role of Activation Function in Neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3  Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Logistic regression use a logistic function, perceptron use a step function. The output of logistic regression is continuous. The output of perceptron is discrete.\n",
    "\n",
    "2. \n",
    "- Advantages\n",
    "  - It easy to use, and no need to manually design features.\n",
    "  - It can learn continuously. The performance improves with the increase of the number of iterations.\n",
    "- Disadvantages\n",
    "  - It has low interpretability.\n",
    "  - It requires large dataset to train.\n",
    "  - It is easy to overfit.\n",
    "\n",
    "3. It can make Neural network to be non-linear.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9cf8428aa180ee23632ed7df20f7a595edda7c60e668686876baf89d702ea1cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
